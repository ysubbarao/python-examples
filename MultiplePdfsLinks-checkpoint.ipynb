{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.ntsb.gov/_layouts/ntsb.aviation/AccList.aspx?month=1&year=2019\"\n",
    "\n",
    "\n",
    "request = urllib3.Request(url)\n",
    "html = urllib3.urlopen(request)\n",
    "soup = BeautifulSoup(html.read())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "\n",
    "http = urllib3.PoolManager()\n",
    "\n",
    "url = 'http://www.thefamouspeople.com/singers.php'\n",
    "response = http.request('GET', url)\n",
    "soup = BeautifulSoup(response.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for j in range(1,13):\n",
    "    \n",
    "    for i in range(1962,2020):\n",
    "        \n",
    "        url = \"https://www.ntsb.gov/_layouts/ntsb.aviation/AccList.aspx?month=%i&year=%i\"%(j,i) \n",
    "        \n",
    "        request = urllib2.Request(url)\n",
    "        html = urllib2.urlopen(request)\n",
    "        soup = BeautifulSoup(html.read())\n",
    "        \n",
    "        for tag in soup.findAll('a', href = True):\n",
    "\n",
    "        \n",
    "\n",
    "        tag['href'] = urlparse.urljoin(url, tag['href'])\n",
    "\n",
    "\n",
    "        if os.path.sp1itext(os.path.basename(tag[ 'href']))[1] = '.pdf':\n",
    "\n",
    "            Current = urllibl.urlopen(tag['href'])\n",
    "\n",
    "            # Convert text into keywords\n",
    "            raw = Current.read()\n",
    "            #The word_tokenize() function will break our text phrases into #individual words\n",
    "            tokens = word_tokenize(raw['content'])\n",
    "\n",
    "            #we'll create a new list which contains punctuation we wish to clean\n",
    "            punctuations = ['(',')',';',':','[',']',',','\\n','...','\"']\n",
    "            #We initialize the stopwords variable which is a list of words like #\"The\", \"I\", \"and\", etc. that don't hold much value as keywords\n",
    "            stop_words = stopwords.words('english')\n",
    "            #We create a list comprehension which only returns a list of words #that are NOT IN stop_words and NOT IN punctuations.\n",
    "\n",
    "            keywords = [word for word in tokens if not word in stop_words and not word in punctuations]\n",
    "            \n",
    "            #\n",
    "            # SEARCH HOURS_OF_FLIGHT_TIME \n",
    "\n",
    "            #at page 13 of the file\n",
    "            HOURS_OF_FLIGHT_TIME = 0\n",
    "\n",
    "            for pos,s in enumerate(keywords) : \n",
    "\n",
    "                if (s=='flight' and keywords[pos-1]=='hours' and keywords[pos+1]=='time'):\n",
    "\n",
    "                    HOURS_OF_FLIGHT_TIME = keywords[pos-2]\n",
    "\n",
    "            HOURS_OF_FLIGHT_TIME    #verify at page 13 of the file \n",
    "\n",
    "            # SEARCH CAPTAIN AGE\n",
    "\n",
    "            for pos,s in enumerate(keywords) : \n",
    "                if (s=='age' and keywords[pos-1]=='captain'):\n",
    "\n",
    "                    CAPTAIN_AGE = keywords[pos+1]\n",
    "\n",
    "            CAPTAIN_AGE        #verify at page 10 of the file\n",
    "            \n",
    "            #Search for the location : \n",
    "            #Location: Casa Grande, AZ \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Use this code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for tag in soup.findAll('a', href = True):\n",
    "\n",
    "        \n",
    "\n",
    "tag['href'] = urlparse.urljoin(url, tag['href'])\n",
    "\n",
    "                     \n",
    "if os.path.sp1itext(os.path.basename(tag[ 'href']))[1] = '.pdf':\n",
    "\n",
    "    Current = urllibl.urlopen(tag['href'])\n",
    "\n",
    "\n",
    "    print(\"\\n[*] Downloading: %s\" %(os.path.basanama(tag['href'])))\n",
    "\n",
    "    f = open(download_path + \"\\\\\" +os.patn.basenane(tag['href'], \"wb\")\n",
    "    f.write(current.read())\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    1+:1\n",
    "\n",
    "print(\"\\n[*] Downloaded xu Filesâ€œ %(i+1))\n",
    "#raw_input(\"[+] Press any key to exit...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
